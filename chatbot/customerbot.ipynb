{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690776f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/tharungopinath/Desktop/Q-A Agentic AI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Project root added to sys.path:\", project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea4ed919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "print(\"GROQ_API_KEY loaded:\", bool(os.getenv(\"GROQ_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "226ca0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base URL: http://localhost:8000\n",
      "Example: factual query ->\n",
      "{'answer': '(tool) Python is a high-level, interpreted programming language known for readability.', 'source': 'tool', 'memory': [{'role': 'user', 'text': 'What is Python?'}, {'role': 'assistant', 'text': '(tool) Python is a high-level, interpreted programming language known for readability.'}]}\n",
      "\n",
      "Example: conversational query ->\n",
      "{'answer': \"I'm functioning properly. How can I assist you today?\", 'source': 'llm', 'memory': [{'role': 'user', 'text': 'What is Python?'}, {'role': 'assistant', 'text': '(tool) Python is a high-level, interpreted programming language known for readability.'}, {'role': 'user', 'text': 'How are you today?'}, {'role': 'assistant', 'text': \"I'm functioning properly. How can I assist you today?\"}]}\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "BASE = os.getenv(\"QA_AGENT_URL\", \"http://localhost:8000\")\n",
    "print(\"Base URL:\", BASE)\n",
    "\n",
    "def send(message, user_id=\"notebook\"):\n",
    "    r = requests.post(f\"{BASE}/chat\", json={\"message\": message, \"user_id\": user_id})\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "print(\"Example: factual query ->\")\n",
    "print(send(\"What is Python?\"))\n",
    "\n",
    "print(\"\\nExample: conversational query ->\")\n",
    "print(send(\"How are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "915cb0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'cleared', 'user_id': 'notebook'}\n"
     ]
    }
   ],
   "source": [
    "def clear_mem(user_id=\"notebook\"):\n",
    "    r = requests.post(f\"{BASE}/clear_memory\", params={\"user_id\": user_id})\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "print(clear_mem(\"notebook\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb7985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.fact_detection import is_factual, small_search_tool\n",
    "from agents.interface import answer_question\n",
    "from typing import Dict, Any\n",
    "\n",
    "def chatbot_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    raw = state.get(\"messages\", \"\")\n",
    "\n",
    "    if isinstance(raw, list):\n",
    "        query = raw[-1] if raw else \"\"\n",
    "        if not isinstance(query, str):\n",
    "            query = str(query)\n",
    "    else:\n",
    "        query = str(raw)\n",
    "\n",
    "    try:\n",
    "        if is_factual(query):\n",
    "            tool_ans = small_search_tool(query)\n",
    "            if tool_ans:\n",
    "                return {\"messages\": f\"(tool) {tool_ans}\"}\n",
    "    except Exception:\n",
    "        pass  \n",
    "\n",
    "    try:\n",
    "        reply = answer_question(query, memory=[])\n",
    "        reply_text = reply if isinstance(reply, str) else str(reply)\n",
    "    except Exception as e:\n",
    "        reply_text = f\"LLM error: {e}\"\n",
    "\n",
    "    return {\"messages\": reply_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b28067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled graph type: <class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "Saved mermaid PNG to static/agentic_graph_two_node.png\n",
      "\n",
      "== quick invokes ==\n",
      "Billing-like test -> (should route to technical if factual): That's correct. FastAPI is known for its high-performance capabilities, scalability, and ease of use. It supports Python 3.7+ and is built on standard Python type hints, making it a popular choice for building APIs quickly and efficiently.\n",
      "Conversational test -> (should route to llm): That's a classic joke. The humor comes from the double meaning of \"make up\" - both referring to atoms being the basic building blocks of matter and \"make up\" as in to fabricate or lie. Scientists do trust atoms, they're a fundamental part of chemistry and physics.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from agents import fact_detection as fact_mod  \n",
    "from agents import interface as interface_mod   \n",
    "from agents import memory as memory_mod        \n",
    "\n",
    "is_factual = getattr(fact_mod, \"is_factual\", None)\n",
    "small_search_tool = getattr(fact_mod, \"small_search_tool\", None)\n",
    "answer_question = getattr(interface_mod, \"answer_question\", None)\n",
    "Memory = getattr(memory_mod, \"Memory\", None)\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list  \n",
    "    intent: str     \n",
    "\n",
    "\n",
    "def router_decision(state: Dict[str, Any]) -> str:\n",
    "    raw = state.get(\"messages\", \"\")\n",
    "    if isinstance(raw, list) and raw:\n",
    "        q = str(raw[-1]).lower()\n",
    "    else:\n",
    "        q = str(raw).lower()\n",
    "\n",
    "    try:\n",
    "        if is_factual and is_factual(q):\n",
    "            return \"technical_node\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"llm_node\"\n",
    "\n",
    "\n",
    "def interface_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    raw = state.get(\"messages\", \"\")\n",
    "    query = raw[-1] if isinstance(raw, list) and raw else raw or \"\"\n",
    "    query = str(query)\n",
    "\n",
    "    \n",
    "    reply = \"\"\n",
    "    try:\n",
    "        if answer_question:\n",
    "            \n",
    "            reply = answer_question(query, memory=[])\n",
    "            \n",
    "            if isinstance(reply, str) and len(reply) > 300:\n",
    "                reply = reply.split(\"\\n\")[0] + \"...\"\n",
    "    except Exception:\n",
    "        reply = \"\"\n",
    "\n",
    "    intent = router_decision({\"messages\": query})\n",
    "    return {\"messages\": reply, \"intent\": intent}\n",
    "\n",
    "\n",
    "def technical_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    raw = state.get(\"messages\", \"\")\n",
    "    \n",
    "    query = raw if isinstance(raw, str) else (raw[-1] if isinstance(raw, list) and raw else \"\")\n",
    "    query = str(query)\n",
    "\n",
    "   \n",
    "    try:\n",
    "        if small_search_tool:\n",
    "            tool_ans = small_search_tool(query)\n",
    "            if tool_ans:\n",
    "                return {\"messages\": f\"(tool) {tool_ans}\"}\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    try:\n",
    "        if answer_question:\n",
    "            return {\"messages\": answer_question(query, memory=[])}\n",
    "    except Exception as e:\n",
    "        return {\"messages\": f\"LLM error: {e}\"}\n",
    "\n",
    "    return {\"messages\": \"technical: no handler available.\"}\n",
    "\n",
    "\n",
    "def llm_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    raw = state.get(\"messages\", \"\")\n",
    "    query = raw if isinstance(raw, str) else (raw[-1] if isinstance(raw, list) and raw else \"\")\n",
    "    query = str(query)\n",
    "\n",
    "    try:\n",
    "        if answer_question:\n",
    "            return {\"messages\": answer_question(query, memory=[])}\n",
    "    except Exception as e:\n",
    "        return {\"messages\": f\"LLM error: {e}\"}\n",
    "\n",
    "    return {\"messages\": \"llm: no handler available.\"}\n",
    "\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"interface_node\", interface_node)\n",
    "graph_builder.add_node(\"fact_detection_node\", technical_node)\n",
    "graph_builder.add_node(\"llm_node\", llm_node)\n",
    "\n",
    "\n",
    "graph_builder.add_edge(START, \"interface_node\")\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"interface_node\",\n",
    "    lambda st: st.get(\"intent\", \"llm_node\"),\n",
    "    {\n",
    "        \"technical_node\": \"technical_node\",\n",
    "        \"llm_node\": \"llm_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"technical_node\", END)\n",
    "graph_builder.add_edge(\"llm_node\", END)\n",
    "\n",
    "\n",
    "try:\n",
    "    graph = graph_builder.compile()\n",
    "except Exception:\n",
    "    graph = graph_builder  \n",
    "\n",
    "print(\"Compiled graph type:\", type(graph))\n",
    "\n",
    "try:\n",
    "    png = graph.get_graph().draw_mermaid_png()\n",
    "    from pathlib import Path\n",
    "    out = Path(\"..\") / \"static\"\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    (out / \"agentic_graph_two_node.png\").write_bytes(png)\n",
    "    print(\"Saved mermaid PNG to static/agentic_graph_two_node.png\")\n",
    "except Exception as e:\n",
    "    print(\"Could not auto-render mermaid PNG (langgraph version may differ):\", e)\n",
    "\n",
    "\n",
    "print(\"\\n== quick invokes ==\")\n",
    "try:\n",
    "    print(\"Billing-like test -> (should route to technical if factual):\",\n",
    "          graph.invoke({\"messages\": \"What is FastAPI?\"}).get(\"messages\"))\n",
    "except Exception as e:\n",
    "    print(\"invoke failed:\", e)\n",
    "\n",
    "try:\n",
    "    print(\"Conversational test -> (should route to llm):\",\n",
    "          graph.invoke({\"messages\": \"Tell me a joke.\"}).get(\"messages\"))\n",
    "except Exception as e:\n",
    "    print(\"invoke failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
